{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find largest number in array\n",
    "\n",
    "def findmax(array):\n",
    "    largestSoFar=array[0]\n",
    "    print('Begins at : ', largestSoFar)\n",
    "    for i in array:\n",
    "        if i > largestSoFar:\n",
    "            largestSoFar=i\n",
    "        print('largestSoFar :', largestSoFar, 'arrayElement :', i)\n",
    "    return('Largest',largestSoFar)\n",
    "\n",
    "findmax([3,4,69,5,93,6])\n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count elements in array:\n",
    "def countEle(array):\n",
    "    count=0\n",
    "    for i in array:\n",
    "        count=count+1\n",
    "    return count\n",
    "\n",
    "countEle([1,2,4,5,2,0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of array\n",
    "\n",
    "def sumEle(array):\n",
    "    sum=0\n",
    "    for i in array:\n",
    "        sum+=i\n",
    "    return sum\n",
    "\n",
    "sumEle([1,2,4,5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find average\n",
    "def avgEle(array):\n",
    "    avg=sumEle(array)/countEle(array)\n",
    "    return avg\n",
    "avgEle([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create array with No>x\n",
    "import numpy as np\n",
    "\n",
    "def biggerThanFun(array,biggerThan):\n",
    "    newarray=np.zeros(len(array))\n",
    "    count=0\n",
    "    for i in array:\n",
    "        if i > biggerThan:\n",
    "            newarray[count]=i\n",
    "            count+=1\n",
    "    print ('Elements of ',array , ' Bigger Than ', biggerThan , ':', newarray[:count])\n",
    "\n",
    "biggerThanFun([1,2,3,4,5,7],4)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find location of element\n",
    "import numpy as np\n",
    "\n",
    "def findEle(array,element):\n",
    "    event=0\n",
    "    foundEv=0\n",
    "    jev=np.zeros(len(array))\n",
    "    j=np.zeros(len(array))\n",
    "    for i in array:\n",
    "        if i == element:\n",
    "            j[event]=1\n",
    "            jev[foundEv]=event\n",
    "            foundEv+=1\n",
    "        event+=1\n",
    "        jav=jev[:foundEv]\n",
    "    print ('The ', element,\"'s are in locations\", jav )\n",
    "\n",
    "findEle([1,2,3,2,4,5,2],2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find e-mail host from sentence\n",
    "def findEmailHost(sentence):\n",
    "    atsign=sentence.find('@')\n",
    "    nextspace=sentence.find(' ', atsign)\n",
    "    host=sentence[(atsign+1):nextspace]\n",
    "    return host\n",
    "\n",
    "findEmailHost('from patrici.rivera@uct.ac.za Sat Jan 42 2030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text file to play with later\n",
    "f= open(\"sampletext.txt\",\"w+\")\n",
    "for i in range(10):\n",
    "    f.write(\"This is line %d\\r\\n\" % (i+1))\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count lines in a textfile\n",
    "def lineCount(file):\n",
    "    fhand=open(file)\n",
    "    count=0\n",
    "    for line in fhand:\n",
    "        count+=1\n",
    "    print(\"The file has\", count, \"lines\")\n",
    "\n",
    "lineCount(\"sampletext.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read txt file\n",
    "fhand=open('mbox.txt')\n",
    "inp=fhand.read()\n",
    "print(inp[:570])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find lines that start with\n",
    "def startWith(file,startw):\n",
    "    try:\n",
    "        fhand=open(file)\n",
    "    except:\n",
    "        print('errrorr file',file,'can;t opent')\n",
    "        quit()\n",
    "        \n",
    "    for line in fhand:\n",
    "        if line.startswith(startw):\n",
    "            print(line.rstrip())\n",
    "            \n",
    "startWith('mbox.txt','From:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=list()\n",
    "r2=str()\n",
    "dir(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list\n",
    "crlist=list() #empty\n",
    "crlist.append('book')\n",
    "crlist.append(66)\n",
    "print(crlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type numbers, type done, and calc avg\n",
    "numlist=list()\n",
    "while True:\n",
    "    num=input('Enter Number, or \"done\"')\n",
    "    if num=='done':\n",
    "        break\n",
    "    value=float(num)\n",
    "    numlist.append(value)\n",
    "\n",
    "average=sum(numlist)/len(numlist)\n",
    "print('Average :',average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with different method (less memory usage)\n",
    "total=0\n",
    "count=0\n",
    "while True:\n",
    "    inp=input('Enter number, or \"done\"')\n",
    "    if inp == 'done': \n",
    "        break\n",
    "    total+=float(value)\n",
    "    count+=1\n",
    "average=total/count\n",
    "print('Average :', average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate words\n",
    "sentence = \"This is a sentence\"\n",
    "stuff = sentence.split()\n",
    "print(stuff,'\\n',stuff[0])\n",
    "\n",
    "print('\\n next is for loop \\n')\n",
    "\n",
    "for w in stuff:\n",
    "    print (w)\n",
    "\n",
    "splitsemicolon=\"This;is;a;colo\"\n",
    "stuff2=splitsemicolon.split(';')\n",
    "print('\\n', stuff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find From and then date from mbox\n",
    "def ffad(file):\n",
    "    fhand=open(file)\n",
    "    for line in fhand:\n",
    "        line=line.strip()\n",
    "        words=line.split()\n",
    "        if len(words)<2:\n",
    "            continue\n",
    "        atsign=words[1].find('@')\n",
    "        if words[0] == 'From':\n",
    "            if atsign!=-1:\n",
    "                print('Day :',words[2])\n",
    "                continue\n",
    "        \n",
    "ffad('mbox.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Dictionaries\n",
    "# like lists but labeled. Keys are different [0],[1] -> ['course'],['tissue']\n",
    "purse=dict()\n",
    "purse['money']=12\n",
    "purse['candy']=3\n",
    "purse['tissues']=75\n",
    "print('1st Purse :', purse)\n",
    "purse['candy']=purse['candy']+2\n",
    "print('2nd Purse :', purse)\n",
    "jjj={'patricio' : 1, 'rivera' : 42, 'jan' :666}\n",
    "print('jjj :',jjj )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count ele frequency in list and put result in dictionary\n",
    "\n",
    "wordlist=['dog','cat','mouse','dog','monkey','dog','horse','cat','moror','dog','monkey']\n",
    "eledic={}\n",
    "no_overlap_list=list()\n",
    "count=0\n",
    "for temp_word in wordlist:\n",
    "    no_overlap_list.append(temp_word)\n",
    "    overlap_check=no_overlap_list.count(temp_word)\n",
    "    if overlap_check !=1:\n",
    "        continue\n",
    "    temp_count=wordlist.count(temp_word)\n",
    "    eledic.update({temp_word : temp_count})\n",
    "    count+=1\n",
    "    print('Es gibt' , temp_count, temp_word )\n",
    "print('eledic :',eledic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicer way\n",
    "counts={}\n",
    "wordlist=['dog','cat','mouse','dog','monkey','dog','horse','cat','moror','dog','monkey']\n",
    "for animal in wordlist:\n",
    "    if animal not in counts:\n",
    "        counts[animal]=1\n",
    "    else :\n",
    "        counts[animal]= counts[animal]+1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even nicer wtf\n",
    "counts={}\n",
    "wordlist=['dog','cat','mouse','dog','monkey','dog','horse','cat','moror','dog','monkey']\n",
    "for animale in wordlist:\n",
    "    counts[animale]=counts.get(animale,0)+1\n",
    "print(counts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting pattern in doc\n",
    "counts=dict()\n",
    "fhand=open('words.txt')\n",
    "fulldoc=fhand.read()\n",
    "words=fulldoc.split()\n",
    "for word in words:\n",
    "    counts[word]=counts.get(word,0)+1\n",
    "print('counts :',counts)\n",
    "sortedwords={k: v for k, v in sorted(counts.items(), key=lambda item: item[1])}\n",
    "print('\\n sorted_words :', sortedwords)\n",
    "understandingsort=sorted(counts.items(), key=lambda item:item[1])\n",
    "print('\\n understanding the sort :', understandingsort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuples have () instead of [], pretty much they are lists\n",
    "# difference: they cannot be modified\n",
    "\n",
    "# read lines from file, make dict of words, split, count each word,\n",
    "fhand = open ('romeo.txt')\n",
    "counts={}\n",
    "for line in fhand:\n",
    "    words=line.split()\n",
    "    for word in words:\n",
    "        counts[word]=counts.get(word,0)+1\n",
    "#make list, loop through k,v, make tupple (v,k), list of tupples, sort\n",
    "\n",
    "#1st way\n",
    "wortdict={k: v for k,v in sorted(counts.items(), key=lambda item:item[1])}\n",
    "\n",
    "tmp=list()\n",
    "for k,v in wortdict.items():\n",
    "    newt=(v,k)\n",
    "    tmp.append(newt)\n",
    "sorttmp=sorted(tmp)\n",
    "\n",
    "#2rd way\n",
    "#sortir=sorted(counts.items(), key=lambda item:item[1])\n",
    "sortor=sorted([ (v,k) for k,v in counts.items()],reverse=True)\n",
    "\n",
    "print ('\\n sorted? dict :',wortdict)\n",
    "print ('\\n flipped tmp : ',tmp)\n",
    "print ('\\n sorted tmp :',sorttmp)\n",
    "#print ('\\n sortir list :',sortir)\n",
    "print ('\\n sortor list :',sortor )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions\n",
    "import re\n",
    "x='My dog is 1 meter and 20 seconds tall, and it is 1 little bitch 666'\n",
    "x2='From: using the: character'\n",
    "x3='From patricio.rivera@ucole.olo.com Sat Jan 32 2030 $50.00 is dalar'\n",
    "\n",
    "y1= re.findall('[0-9]',x)\n",
    "y2= re.findall('[0-9]+',x)\n",
    "y3= re.findall('[MAeIOU]+',x) # some letters\n",
    "y4= re.findall('^F.+?:',x2) #? not greedy\n",
    "y5= re.findall('^F.+:',x2) # greedy\n",
    "y6= re.findall('\\S+@\\S+',x3) # \\S: non-blanc char , +: 1 or more of \\S, with some @ inbetween\n",
    "y7= re.findall('^From (\\S+@\\S+)',x3) # ^:start with 'From ' , (): that's what we want\n",
    "y8= re.findall('^From \\S+@(\\S+)',x3) # find host ||or @([^ ]*) : [^ ] is non-blanc char, * multiple times\n",
    "hand=open('mbox.txt')\n",
    "numlist=list()\n",
    "for line in hand:\n",
    "    line=line.strip()\n",
    "    y9= re.findall('^X-DSPAM-Confidence: ([0-9.]+)',line)\n",
    "    if len(y9) !=1: continue\n",
    "    num=float(y9[0])\n",
    "    numlist.append(num)\n",
    "\n",
    "y10= re.findall('\\$[0-9.]+',x3) # \\$ to find dollar, . takes into account .00\n",
    "\n",
    "print('y1 :', y1)\n",
    "print('y2 :', y2)\n",
    "print('y3 :', y3)\n",
    "print('y4 :', y4)\n",
    "print('y5 :', y5)\n",
    "print('y6 :', y6)\n",
    "print('y7 :', y7)\n",
    "print('y8 :', y8)\n",
    "print('Maximum :', max(numlist))\n",
    "print('y10 :', y10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Services\n",
    "# Create web browser\n",
    "\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n",
    "mysock.connect(('data.pr4e.org',80)) #connect to data.pr4e.org, port:80\n",
    "cmd = 'GET /romeo.txt HTTP/1.0\\r\\nHost: data.pr4e.org\\r\\n\\r\\n'.encode() #what is HTTP1? || encode:prepares data to go across the internet\n",
    "mysock.send(cmd) #send\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512) # receive 512 chars each time\n",
    "    if (len(data)<1):  # 0 chars means end of string, connection is closed\n",
    "        break\n",
    "    print(data.decode()) # we have to decode the data (whatever that means)\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so much better\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand=urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=dict()\n",
    "words=line.decode().split()\n",
    "for word in words:\n",
    "    counts[word]=counts.get(word,0)+1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Web Page\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand=urllib.request.urlopen('https://www.investopedia.com/terms/m/mortgage-company.asp')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter - ')\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tags=soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href',None))\n",
    "# Enter : http://www.dr-chuck.com/page1.htm\n",
    "# Result: http://www.dr-chuck.com/page2.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGAIN (Remove possible errors: for certificates that are not in pythons official list)\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "#ignore ssl certificate errors\n",
    "ctx=ssl.create_default_context()\n",
    "ctx.check_hostname=False\n",
    "ctx.verify_mode=ssl.CERT_NONE\n",
    "\n",
    "url=input('Enter - ')\n",
    "html=urllib.request.urlopen(url, context=ctx).read()\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "#retrieve anchor tags\n",
    "tags=soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href',None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Services\n",
    "# XML\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "data = '''\n",
    "<person>\n",
    "  <name>Patricio</name>\n",
    "  <phone type=\"intl\">\n",
    "    +1 734 303 4456\n",
    "  </phone>\n",
    "  <email hide=\"yes\" />\n",
    "</person>'''\n",
    "\n",
    "tree = ET.fromstring(data)\n",
    "print('Name:', tree.find('name').text)\n",
    "print('Attr:', tree.find('email').get('hide'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "input = '''\n",
    "<stuff>\n",
    "  <users>\n",
    "    <user x=\"2\">\n",
    "      <id>001</id>\n",
    "      <name>Butch</name>\n",
    "    </user>\n",
    "    <user x=\"7\">\n",
    "      <id>009</id>\n",
    "      <name>Brent</name>\n",
    "    </user>\n",
    "  </users>\n",
    "</stuff>'''\n",
    "\n",
    "stuff = ET.fromstring(input)\n",
    "lst = stuff.findall('users/user')\n",
    "print('User count:', len(lst))\n",
    "\n",
    "for item in lst:\n",
    "    print('Name', item.find('name').text)\n",
    "    print('Id', item.find('id').text)\n",
    "    print('Attribute', item.get('x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python objects classes and sht\n",
    "class partyanimal:\n",
    "    x=0\n",
    "    def party(self):\n",
    "        self.x = self.x+1\n",
    "        print(\"so far\",self.x)\n",
    "an=partyanimal()\n",
    "\n",
    "an.party()\n",
    "an.party()\n",
    "an.x\n",
    "print('\\n tpye :',type(an))\n",
    "print('\\n dir :', dir(an))\n",
    "print('\\n type party :',type(an.party))\n",
    "print('\\n dir perty :', dir(an.party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructors and destructors\n",
    "class partyanim:\n",
    "    x=0\n",
    "    \n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        print('i am constructed')\n",
    "    def party(self):\n",
    "        self.x=self.x+1\n",
    "        print('so far', self.x)\n",
    "        \n",
    "    def __del__(self):\n",
    "        print('im destructed',self.x)\n",
    "\n",
    "am=partyanim()\n",
    "am.party()\n",
    "am.party()\n",
    "print(am)\n",
    "am=42\n",
    "print('am contains :',am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let;s home it makes sense here\n",
    "\n",
    "class partyanim:\n",
    "    x=0\n",
    "    name=\"\"\n",
    "    \n",
    "    def __init__(self,z):\n",
    "        print('z :',z)\n",
    "        print('self :',self)\n",
    "        self.name=z\n",
    "        print('selftype',type(self.name))\n",
    "        print(self.name,'constructed')\n",
    "    def party(self):\n",
    "        self.x=self.x+1\n",
    "        print(self.name,'partycount', self.x)\n",
    "    \n",
    "\n",
    "s=partyanim(42)\n",
    "j=partyanim(\"jimmi\")\n",
    "s.party()\n",
    "s.party()\n",
    "j.party()\n",
    "print('s.x :',s.x)\n",
    "print('s.name :', s.name)\n",
    "print('s contains :',s)\n",
    "print('\\n dirs :\\n',dir(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class partyanim:\n",
    "    x=0\n",
    "    name=\"\"\n",
    "    def __init__(self,z):\n",
    "        self.name=z\n",
    "        print(self.name,'constructed')\n",
    "    def party(self):\n",
    "        self.x=self.x+1\n",
    "        print(self.name,'partycount', self.x)\n",
    "\n",
    "class futbolFan(partyanim):\n",
    "    points=0\n",
    "    def touch(self):\n",
    "        self.points=self.points+7\n",
    "        self.party()\n",
    "        print(self.name,'points', self.points)\n",
    "\n",
    "s=partyanim('sali')\n",
    "s.party()\n",
    "j=futbolFan(\"jimi\")\n",
    "j.party()\n",
    "j.touch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL!!!\n",
    "# create read update delete\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('emaildb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('DROP TABLE IF EXISTS Counts')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE Counts (email TEXT, count INTEGER)''')\n",
    "\n",
    "fname = input('Enter file name: ')\n",
    "if (len(fname) < 1): fname = 'mbox-short.txt'\n",
    "fh = open(fname)\n",
    "for line in fh:\n",
    "    if not line.startswith('From: '): continue\n",
    "    pieces = line.split()\n",
    "    email = pieces[1]\n",
    "    cur.execute('SELECT count FROM Counts WHERE email = ? ', (email,))\n",
    "    row = cur.fetchone() #grab first one\n",
    "    if row is None:\n",
    "        cur.execute('''INSERT INTO Counts (email, count)\n",
    "                VALUES (?, 1)''', (email,))\n",
    "    else:\n",
    "        cur.execute('UPDATE Counts SET count = count + 1 WHERE email = ?',\n",
    "                    (email,))\n",
    "    conn.commit()\n",
    "\n",
    "# https://www.sqlite.org/lang_select.html\n",
    "sqlstr = 'SELECT email, count FROM Counts ORDER BY count DESC LIMIT 10'\n",
    "\n",
    "for row in cur.execute(sqlstr):\n",
    "    print(str(row[0]), row[1])\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter spider - find newest friends /of friends /of friends\n",
    "from urllib.request import urlopen\n",
    "import urllib.error\n",
    "import twurl\n",
    "import json\n",
    "import sqlite3\n",
    "import ssl\n",
    "\n",
    "TWITTER_URL = 'https://api.twitter.com/1.1/friends/list.json'\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS Twitter\n",
    "            (name TEXT, retrieved INTEGER, friends INTEGER)''')\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    acct = input('Enter a Twitter account, or quit: ')\n",
    "    #acct = 'PatricioRiveera'\n",
    "    if (acct == 'quit'): break\n",
    "    if (len(acct) < 1):\n",
    "        cur.execute('SELECT name FROM Twitter WHERE retrieved = 0 LIMIT 1')\n",
    "        try:\n",
    "            acct = cur.fetchone()[0]\n",
    "        except:\n",
    "            print('No unretrieved Twitter accounts found')\n",
    "            continue\n",
    "\n",
    "    url = twurl.augment(TWITTER_URL, {'screen_name': acct, 'count': '5'})\n",
    "    print('Retrieving', url)\n",
    "    connection = urlopen(url, context=ctx)\n",
    "    data = connection.read().decode()\n",
    "    headers = dict(connection.getheaders())\n",
    "\n",
    "    print('Remaining', headers['x-rate-limit-remaining'])\n",
    "    js = json.loads(data)\n",
    "    # Debugging\n",
    "    # print json.dumps(js, indent=4)\n",
    "\n",
    "    cur.execute('UPDATE Twitter SET retrieved=1 WHERE name = ?', (acct, ))\n",
    "\n",
    "    countnew = 0\n",
    "    countold = 0\n",
    "    for u in js['users']:\n",
    "        friend = u['screen_name']\n",
    "        print(friend)\n",
    "        cur.execute('SELECT friends FROM Twitter WHERE name = ? LIMIT 1',\n",
    "                    (friend, ))\n",
    "        try:\n",
    "            count = cur.fetchone()[0]\n",
    "            cur.execute('UPDATE Twitter SET friends = ? WHERE name = ?',\n",
    "                        (count+1, friend))\n",
    "            countold = countold + 1\n",
    "        except:\n",
    "            cur.execute('''INSERT INTO Twitter (name, retrieved, friends)\n",
    "                        VALUES (?, 0, 1)''', (friend, ))\n",
    "            countnew = countnew + 1\n",
    "    print('New accounts=', countnew, ' revisited=', countold)\n",
    "    conn.commit()\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases exer\n",
    "import xml.etree.ElementTree as ET\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('trackdb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Make some fresh tables using executescript()\n",
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS Artist;\n",
    "DROP TABLE IF EXISTS Album;\n",
    "DROP TABLE IF EXISTS Track;\n",
    "\n",
    "CREATE TABLE Artist (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name    TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Album (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    artist_id  INTEGER,\n",
    "    title   TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Track (\n",
    "    id  INTEGER NOT NULL PRIMARY KEY \n",
    "        AUTOINCREMENT UNIQUE,\n",
    "    title TEXT  UNIQUE,\n",
    "    album_id  INTEGER,\n",
    "    len INTEGER, rating INTEGER, count INTEGER\n",
    ");\n",
    "''')\n",
    "\n",
    "\n",
    "fname = input('Enter file name: ')\n",
    "if ( len(fname) < 1 ) : fname = 'Library.xml'\n",
    "\n",
    "# <key>Track ID</key><integer>369</integer>\n",
    "# <key>Name</key><string>Another One Bites The Dust</string>\n",
    "# <key>Artist</key><string>Queen</string>\n",
    "def lookup(d, key):\n",
    "    found = False\n",
    "    for child in d:\n",
    "        if found : return child.text\n",
    "        if child.tag == 'key' and child.text == key :\n",
    "            found = True\n",
    "    return None\n",
    "\n",
    "stuff = ET.parse(fname)\n",
    "all = stuff.findall('dict/dict/dict')\n",
    "print('Dict count:', len(all))\n",
    "for entry in all:\n",
    "    if ( lookup(entry, 'Track ID') is None ) : continue\n",
    "\n",
    "    name = lookup(entry, 'Name')\n",
    "    artist = lookup(entry, 'Artist')\n",
    "    album = lookup(entry, 'Album')\n",
    "    count = lookup(entry, 'Play Count')\n",
    "    rating = lookup(entry, 'Rating')\n",
    "    length = lookup(entry, 'Total Time')\n",
    "\n",
    "    if name is None or artist is None or album is None : \n",
    "        continue\n",
    "\n",
    "    print(name, artist, album, count, rating, length)\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Artist (name) \n",
    "        VALUES ( ? )''', ( artist, ) )\n",
    "    cur.execute('SELECT id FROM Artist WHERE name = ? ', (artist, ))\n",
    "    artist_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Album (title, artist_id) \n",
    "        VALUES ( ?, ? )''', ( album, artist_id ) )\n",
    "    cur.execute('SELECT id FROM Album WHERE title = ? ', (album, ))\n",
    "    album_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR REPLACE INTO Track\n",
    "        (title, album_id, len, rating, count) \n",
    "        VALUES ( ?, ?, ?, ?, ? )''', \n",
    "        ( name, album_id, length, rating, count ) )\n",
    "\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to many relationships\n",
    "# Unique combinations\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('rosterdb.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Do some setup\n",
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS User;\n",
    "DROP TABLE IF EXISTS Member;\n",
    "DROP TABLE IF EXISTS Course;\n",
    "\n",
    "CREATE TABLE User (\n",
    "    id     INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    name   TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Course (\n",
    "    id     INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    title  TEXT UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Member (\n",
    "    user_id     INTEGER,\n",
    "    course_id   INTEGER,\n",
    "    role        INTEGER,\n",
    "    PRIMARY KEY (user_id, course_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "fname = input('Enter file name: ')\n",
    "if len(fname) < 1:\n",
    "    fname = 'roster_data_sample.json'\n",
    "\n",
    "# [\n",
    "#   [ \"Charley\", \"si110\", 1 ],\n",
    "#   [ \"Mea\", \"si110\", 0 ],\n",
    "\n",
    "str_data = open(fname).read()\n",
    "json_data = json.loads(str_data)\n",
    "\n",
    "for entry in json_data:\n",
    "\n",
    "    name = entry[0];\n",
    "    title = entry[1];\n",
    "\n",
    "    print((name, title))\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO User (name)\n",
    "        VALUES ( ? )''', ( name, ) )\n",
    "    cur.execute('SELECT id FROM User WHERE name = ? ', (name, ))\n",
    "    user_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR IGNORE INTO Course (title)\n",
    "        VALUES ( ? )''', ( title, ) )\n",
    "    cur.execute('SELECT id FROM Course WHERE title = ? ', (title, ))\n",
    "    course_id = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute('''INSERT OR REPLACE INTO Member\n",
    "        (user_id, course_id) VALUES ( ?, ? )''',\n",
    "        ( user_id, course_id ) )\n",
    "\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twfriends.py\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import twurl\n",
    "import json\n",
    "import sqlite3\n",
    "import ssl\n",
    "\n",
    "TWITTER_URL = 'https://api.twitter.com/1.1/friends/list.json'\n",
    "\n",
    "conn = sqlite3.connect('friends.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS People\n",
    "            (id INTEGER PRIMARY KEY, name TEXT UNIQUE, retrieved INTEGER)''')\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Follows\n",
    "            (from_id INTEGER, to_id INTEGER, UNIQUE(from_id, to_id))''')\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    acct = input('Enter a Twitter account, or quit: ')\n",
    "    if (acct == 'quit'): break\n",
    "    if (len(acct) < 1):\n",
    "        cur.execute('SELECT id, name FROM People WHERE retrieved=0 LIMIT 1')\n",
    "        try:\n",
    "            (id, acct) = cur.fetchone()\n",
    "        except:\n",
    "            print('No unretrieved Twitter accounts found')\n",
    "            continue\n",
    "    else:\n",
    "        cur.execute('SELECT id FROM People WHERE name = ? LIMIT 1',\n",
    "                    (acct, ))\n",
    "        try:\n",
    "            id = cur.fetchone()[0]\n",
    "        except:\n",
    "            cur.execute('''INSERT OR IGNORE INTO People\n",
    "                        (name, retrieved) VALUES (?, 0)''', (acct, ))\n",
    "            conn.commit()\n",
    "            if cur.rowcount != 1:\n",
    "                print('Error inserting account:', acct)\n",
    "                continue\n",
    "            id = cur.lastrowid\n",
    "\n",
    "    url = twurl.augment(TWITTER_URL, {'screen_name': acct, 'count': '100'})\n",
    "    print('Retrieving account', acct)\n",
    "    try:\n",
    "        connection = urllib.request.urlopen(url, context=ctx)\n",
    "    except Exception as err:\n",
    "        print('Failed to Retrieve', err)\n",
    "        break\n",
    "\n",
    "    data = connection.read().decode()\n",
    "    headers = dict(connection.getheaders())\n",
    "\n",
    "    print('Remaining', headers['x-rate-limit-remaining'])\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        print('Unable to parse json')\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "    # Debugging\n",
    "    # print(json.dumps(js, indent=4))\n",
    "\n",
    "    if 'users' not in js:\n",
    "        print('Incorrect JSON received')\n",
    "        print(json.dumps(js, indent=4))\n",
    "        continue\n",
    "\n",
    "    cur.execute('UPDATE People SET retrieved=1 WHERE name = ?', (acct, ))\n",
    "\n",
    "    countnew = 0\n",
    "    countold = 0\n",
    "    for u in js['users']:\n",
    "        friend = u['screen_name']\n",
    "        print(friend)\n",
    "        cur.execute('SELECT id FROM People WHERE name = ? LIMIT 1',\n",
    "                    (friend, ))\n",
    "        try:\n",
    "            friend_id = cur.fetchone()[0]\n",
    "            countold = countold + 1\n",
    "        except:\n",
    "            cur.execute('''INSERT OR IGNORE INTO People (name, retrieved)\n",
    "                        VALUES (?, 0)''', (friend, ))\n",
    "            conn.commit()\n",
    "            if cur.rowcount != 1:\n",
    "                print('Error inserting account:', friend)\n",
    "                continue\n",
    "            friend_id = cur.lastrowid\n",
    "            countnew = countnew + 1\n",
    "        cur.execute('''INSERT OR IGNORE INTO Follows (from_id, to_id)\n",
    "                    VALUES (?, ?)''', (id, friend_id))\n",
    "    print('New accounts=', countnew, ' revisited=', countold)\n",
    "    print('Remaining', headers['x-rate-limit-remaining'])\n",
    "    conn.commit()\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA MINING - DATA ANALYSIS - DATA VISUALIZACION\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import http\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = \"http://py4e-data.dr-chuck.net/json?\"\n",
    "else :\n",
    "    serviceurl = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "\n",
    "# Additional detail for urllib\n",
    "# http.client.HTTPConnection.debuglevel = 1\n",
    "\n",
    "conn = sqlite3.connect('geodata.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Locations (address TEXT, geodata TEXT)''')\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "fh = open(\"geodata/where.data\")\n",
    "count = 0\n",
    "for line in fh:\n",
    "    if count > 200 :\n",
    "        print('Retrieved 200 locations, restart to retrieve more')\n",
    "        break\n",
    "\n",
    "    address = line.strip()\n",
    "    print('')\n",
    "    cur.execute(\"SELECT geodata FROM Locations WHERE address= ?\",\n",
    "        (memoryview(address.encode()), ))\n",
    "\n",
    "    try:\n",
    "        data = cur.fetchone()[0]\n",
    "        print(\"Found in database \",address)\n",
    "        continue\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    parms = dict()\n",
    "    parms[\"address\"] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters', data[:20].replace('\\n', ' '))\n",
    "    count = count + 1\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        print(data)  # We print in case unicode causes an error\n",
    "        continue\n",
    "\n",
    "    if 'status' not in js or (js['status'] != 'OK' and js['status'] != 'ZERO_RESULTS') :\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "    cur.execute('''INSERT INTO Locations (address, geodata)\n",
    "            VALUES ( ?, ? )''', (memoryview(address.encode()), memoryview(data.encode()) ) )\n",
    "    conn.commit()\n",
    "    if count % 10 == 0 :\n",
    "        print('Pausing for a bit...')\n",
    "        time.sleep(5)\n",
    "\n",
    "print(\"Run geodump.py to read the data from the database so you can vizualize it on a map.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# godump.py\n",
    "import sqlite3\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "conn = sqlite3.connect('geodata.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT * FROM Locations')\n",
    "fhand = codecs.open('geodata/where.js', 'w', \"utf-8\")\n",
    "fhand.write(\"myData = [\\n\")\n",
    "count = 0\n",
    "for row in cur :\n",
    "    data = str(row[1].decode())\n",
    "    try: js = json.loads(str(data))\n",
    "    except: continue\n",
    "\n",
    "    if not('status' in js and js['status'] == 'OK') : continue\n",
    "\n",
    "    lat = js[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "    lng = js[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "    if lat == 0 or lng == 0 : continue\n",
    "    where = js['results'][0]['formatted_address']\n",
    "    where = where.replace(\"'\", \"\")\n",
    "    try :\n",
    "        print(where, lat, lng)\n",
    "\n",
    "        count = count + 1\n",
    "        if count > 1 : fhand.write(\",\\n\")\n",
    "        output = \"[\"+str(lat)+\",\"+str(lng)+\", '\"+where+\"']\"\n",
    "        fhand.write(output)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "fhand.write(\"\\n];\\n\")\n",
    "cur.close()\n",
    "fhand.close()\n",
    "print(count, \"records written to where.js\")\n",
    "print(\"Open where.html to view the data in a browser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: http://www.dr-chuck.com/\n",
      "['http://www.dr-chuck.com']\n",
      "How many pages:1\n",
      "1 http://www.dr-chuck.com (8769) 4\n",
      "How many pages:1\n",
      "2 http://www.dr-chuck.com/dr-chuck/resume/index.htm (1850) 11\n",
      "How many pages:100\n",
      "4 http://www.dr-chuck.com/sakai-book (5843) 4\n",
      "13 http://www.dr-chuck.com/dr-chuck/resume/pictures/index.htm (1827) 5\n",
      "20 http://www.dr-chuck.com/dr-chuck/resume/pictures/Sample1926.tif Ignore non text/html page\n",
      "18 http://www.dr-chuck.com/errata.txt Unable to retrieve or parse page\n",
      "9 http://www.dr-chuck.com/dr-chuck/resume/research2017-05.docx Ignore non text/html page\n",
      "12 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm (17460) 2\n",
      "6 http://www.dr-chuck.com/dr-chuck/resume/bio.htm (3073) 3\n",
      "3 http://www.dr-chuck.com/office (7491) 0\n",
      "16 http://www.dr-chuck.com/Sakai_ Building an Open Source Community - Charles R. Severance.PDF Unable to retrieve or parse page\n",
      "22 http://www.dr-chuck.com/dr-chuck/resume/pictures/_HJS1912.tif Ignore non text/html page\n",
      "7 http://www.dr-chuck.com/dr-chuck/resume/resume2018-08.doc Ignore non text/html page\n",
      "21 http://www.dr-chuck.com/dr-chuck/resume/pictures/_HJS1908.tif Ignore non text/html page\n",
      "23 http://www.dr-chuck.com/dr-chuck/resume/pictures/2016-02-jay-jackson/severance_charles_book_cover_080715_neutral_278.tif Ignore non text/html page\n",
      "14 http://www.dr-chuck.com/dr-chuck/resume (1850) 11\n",
      "32 http://www.dr-chuck.com/dr-chuck/pictures/index.htm Unable to retrieve or parse page\n",
      "19 http://www.dr-chuck.com/dr-chuck/resume/pictures/Sample1916.tif Ignore non text/html page\n",
      "17 http://www.dr-chuck.com/html Unable to retrieve or parse page\n",
      "30 http://www.dr-chuck.com/dr-chuck/leadership.htm Unable to retrieve or parse page\n",
      "25 http://www.dr-chuck.com/dr-chuck/bio.htm Unable to retrieve or parse page\n",
      "28 http://www.dr-chuck.com/dr-chuck/research2017-05.docx Unable to retrieve or parse page\n",
      "10 http://www.dr-chuck.com/dr-chuck/resume/teach2017-08.docx Ignore non text/html page\n",
      "5 http://www.dr-chuck.com/obi-sample (2426) 1\n",
      "31 http://www.dr-chuck.com/dr-chuck/speaking.htm Unable to retrieve or parse page\n",
      "26 http://www.dr-chuck.com/dr-chuck/resume2018-08.doc Unable to retrieve or parse page\n",
      "27 http://www.dr-chuck.com/dr-chuck/cv2019-08.doc Unable to retrieve or parse page\n",
      "29 http://www.dr-chuck.com/dr-chuck/teach2017-08.docx Unable to retrieve or parse page\n",
      "11 http://www.dr-chuck.com/dr-chuck/resume/leadership.htm (6168) 1\n",
      "8 http://www.dr-chuck.com/dr-chuck/resume/cv2019-08.doc Ignore non text/html page\n",
      "15 http://www.dr-chuck.com/Sakai_ Building an Open Source Community - Charles R. Severance.epub Unable to retrieve or parse page\n",
      "24 http://www.dr-chuck.com/dr-chuck/resume/pictures (1827) 5\n",
      "34 http://www.dr-chuck.com/dr-chuck/resume/Sample1926.tif Unable to retrieve or parse page\n",
      "35 http://www.dr-chuck.com/dr-chuck/resume/_HJS1908.tif Unable to retrieve or parse page\n",
      "33 http://www.dr-chuck.com/dr-chuck/resume/Sample1916.tif Unable to retrieve or parse page\n",
      "36 http://www.dr-chuck.com/dr-chuck/resume/_HJS1912.tif Unable to retrieve or parse page\n",
      "37 http://www.dr-chuck.com/dr-chuck/resume/2016-02-jay-jackson/severance_charles_book_cover_080715_neutral_278.tif Unable to retrieve or parse page\n",
      "No unretrieved HTML pages found\n"
     ]
    }
   ],
   "source": [
    "# WEB CRAWLING : \n",
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations:10\n",
      "1 0.24044444444444438\n",
      "2 0.10957037037037036\n",
      "3 0.0809353086419752\n",
      "4 0.056714847736625496\n",
      "5 0.036181622496570566\n",
      "6 0.02165626039323255\n",
      "7 0.012169265213839254\n",
      "8 0.006541524314322045\n",
      "9 0.003324879031195904\n",
      "10 0.001609098186718627\n",
      "[(1, 1.9745622678067576), (2, 2.1969085333653355), (4, 0.8524962079197327), (13, 0.5615912342772957), (12, 0.898036919524269)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON output on spider.js...\n",
      "How many nodes? 3\n",
      "Open force.html in a browser to view the visualization\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    map[row[3]] = count\n",
    "    ranks[row[3]] = rank\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
